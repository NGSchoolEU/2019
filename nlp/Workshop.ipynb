{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy as sp\n",
    "import re\n",
    "import pickle as pkl\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import libsvm,SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = sp.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data file and do some preliminary exploration\n",
    "\n",
    "- Use Panda to read the data csv file\n",
    "- Check the top 5 rows in the data frame\n",
    "- Review the unique values of the Category, i.e. the labels\n",
    "- Get the number of samples per class\n",
    "- Sample some Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sampled_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Snippet</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oil and Gas</td>\n",
       "      <td>mining</td>\n",
       "      <td>You are here: Home &gt; Appraisal &gt; Mineral Right...</td>\n",
       "      <td>Oil and Gas Mineral Rights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oil and Gas</td>\n",
       "      <td>extraction</td>\n",
       "      <td>There are many storage wells capable of holdin...</td>\n",
       "      <td>How do you extract oil - Answers.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Gas</td>\n",
       "      <td>field</td>\n",
       "      <td>Aberdeen Exhibition &amp; Conference Centre: GE Oi...</td>\n",
       "      <td>GE Oil and Gas Arena - Aberdeen Exhibition &amp; C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oil and Gas</td>\n",
       "      <td>Producers</td>\n",
       "      <td>The UK offshore oil and gas industry benefits ...</td>\n",
       "      <td>About the Industry | Oil &amp; Gas UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil and Gas</td>\n",
       "      <td>rig</td>\n",
       "      <td>Search CareerBuilder for Oil Truck Driver Jobs...</td>\n",
       "      <td>Oil Truck Driver Jobs - Apply Now | CareerBuilder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category     Keyword                                            Snippet  \\\n",
       "0  Oil and Gas      mining  You are here: Home > Appraisal > Mineral Right...   \n",
       "1  Oil and Gas  extraction  There are many storage wells capable of holdin...   \n",
       "2  Oil and Gas       field  Aberdeen Exhibition & Conference Centre: GE Oi...   \n",
       "3  Oil and Gas   Producers  The UK offshore oil and gas industry benefits ...   \n",
       "4  Oil and Gas         rig  Search CareerBuilder for Oil Truck Driver Jobs...   \n",
       "\n",
       "                                               Title  \n",
       "0                         Oil and Gas Mineral Rights  \n",
       "1               How do you extract oil - Answers.com  \n",
       "2  GE Oil and Gas Arena - Aberdeen Exhibition & C...  \n",
       "3                  About the Industry | Oil & Gas UK  \n",
       "4  Oil Truck Driver Jobs - Apply Now | CareerBuilder  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Oil and Gas', 'Materials', 'Technology', 'Utilities',\n",
       "       'Telecommunications', 'Media', 'Consumer Automobiles',\n",
       "       'Consumer Services', 'Consumer Retail', 'consumer staples',\n",
       "       'financial', 'transportation', 'Aerospace and defense',\n",
       "       'health care', 'construction and engineering',\n",
       "       'commercial and industrial services', 'professional services',\n",
       "       'government', 'Real Estate'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Technology                            1000\n",
       "transportation                        1000\n",
       "financial                             1000\n",
       "government                            1000\n",
       "Aerospace and defense                 1000\n",
       "Real Estate                           1000\n",
       "Oil and Gas                           1000\n",
       "Consumer Retail                       1000\n",
       "commercial and industrial services    1000\n",
       "Media                                 1000\n",
       "professional services                 1000\n",
       "Consumer Services                     1000\n",
       "Telecommunications                    1000\n",
       "health care                           1000\n",
       "Consumer Automobiles                  1000\n",
       "Utilities                             1000\n",
       "consumer staples                      1000\n",
       "construction and engineering          1000\n",
       "Materials                             1000\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SGS provides fuel oil testing services through our global network of laboratories. Find out more.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Snippet'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Antispam warriors are developing new weapons to fight unwanted junk e-mail and legislators are debating strict new laws that could send spammers to jail. A look at 5 ...'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Snippet'][2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numerous studies have suggested that using social media sites such as Facebook and Twitter can affect mental health and well-being. We look at the evidence.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Snippet'][5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "- Remove the NaNs\n",
    "- Combine the string data in a new column after converting all the string to lower case\n",
    "- Clean the text\n",
    "    + remove non_alphabets\n",
    "    + use spaCy model to keep Nouns, Verbs and ProNouns\n",
    "    + use spaCy model to remove non-English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Title.fillna('',inplace=True)\n",
    "df.Snippet.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Doc']=df.Keyword+' '+ df.Title.str.lower()+' '+ df.Snippet.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'garage oil and gas iq oil & gas iq offers authoritative insight and opinion on all aspects affecting the oil and gas industry.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Doc'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_preprocessing(data):\n",
    "    ''' The function processing the data including: keeping only nouns verbs and pronouns, remove extra characters\n",
    "    Args:\n",
    "        data: string\n",
    "    Returns:\n",
    "        string\n",
    "    '''\n",
    "    \n",
    "    data = re.sub('[^a-z]',' ',data)\n",
    "    \n",
    "    doc = nlp(data)\n",
    "    text = []\n",
    "    for word in doc:\n",
    "        if word.pos_ in ['PROPN','NOUN','VERB'] and np.sum(word.vector) !=0:\n",
    "            text.append(word.text)\n",
    "  \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scaffolding sheet material cut to size mdf plywood veneered sheet material ut to size mdf plywood veneered flexi real wood moisture resistant exotic'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Doc'][1004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scaffolding sheet material cut size plywood veneered sheet material ut size plywood veneered wood moisture'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_preprocessing(df['Doc'][1004])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LDA\n",
    "- write functions to build an LDA model and to view the generated topics,\n",
    "- the model uses CountVectorizer as an input to the LDA model\n",
    "- build LDA for the class Technology to demonstrate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lda(data,num_topics):\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(data)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "    lda.fit(tf)\n",
    "    \n",
    "    return lda,tf_vectorizer\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_tech = df[df['Category']=='Technology']['Doc'].apply(lambda x: string_preprocessing(x))\n",
    "data_tech = df['Doc'].apply(lambda x: string_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: car water used protection cars performance treatment air freight systems\n",
      "Topic #1: health care sale services minister homes service tax ministry goods\n",
      "Topic #2: consumer services transportation gas business oil market uk department sector\n",
      "Topic #3: government company investment management food service financial finance network rights\n",
      "Topic #4: services office waste auto act recycling computer building plans luxury\n",
      "Topic #5: estate property materials industrial commercial com house definition properties material\n",
      "Topic #6: state support utility accounting apartment telecommunications advice credit education satellite\n",
      "Topic #7: construction engineering aerospace services media technology management solutions jobs design\n",
      "Topic #8: wikipedia school airport pharmacy tech medicine sport retailers departments music\n",
      "Topic #9: defense staples city index cable tv video etf manager hotel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_technology,tf_vectorizer = build_lda(data_tech,num_topics=10) # change num_topics to see its impact\n",
    "print_top_words(lda_technology,tf_vectorizer.get_feature_names(),n_top_words=10) # change n_top_words to see its impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LDA for every class in the data\n",
    "- Assume all classes require the same number of topics\n",
    "- Clean the text before building the LDA models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 20\n",
    "\n",
    "ldas = []\n",
    "tf_vectorizers = []\n",
    "for cat in df.Category.unique():\n",
    "    cat_df = df['Doc'].where(df['Category'] == cat).dropna(how='any')\n",
    "    cat_df = cat_df.apply(lambda x: string_preprocessing(x))\n",
    "    lda_t,tf_vectorizer_t = build_lda(cat_df,num_topics=n_components)\n",
    "    ldas.append(lda_t)\n",
    "    tf_vectorizers.append(tf_vectorizer_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the LDA models\n",
    "Do not run if you do not want to override the supplied file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tutorial_ldas.pkl','wb') as f:\n",
    "        pkl.dump((ldas,tf_vectorizers),f,pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the LDA models from an already provided file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tutorial_ldas.pkl','rb') as f:\n",
    "       (ldas,tf_vectorizers)= pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Pass the data to every LDA model, extract and combine the topic features\n",
    "- if wordEmbed == True then add the spaCy langauge model embedding to the extracted LDA features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data,ldas,tf_vects, wordEmbed=True):\n",
    "    features=[]\n",
    "    labels =[]\n",
    "    assert(len(ldas)==len(tf_vects))\n",
    "    for i,d in data.iterrows():\n",
    "        labels.append(d['Category'])\n",
    "        line= []\n",
    "        for j in range(len(ldas)):\n",
    "            line.extend(ldas[j].transform(tf_vects[j].transform([d['Doc']]))[0])\n",
    "        if wordEmbed:\n",
    "            line.extend(list(nlp(d['Doc']).vector))\n",
    "        features.append(line)\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,labels=feature_extraction(df.dropna(how='any'),ldas,tf_vectorizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the extracted features\n",
    "Do not run if you do not want to override the supplied file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tutorial_features.pkl','wb') as f:\n",
    "        pkl.dump((features,labels),f,pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the extracted features from the provided file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tutorial_features.pkl','rb') as f:\n",
    "       (features,labels)= pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for classification\n",
    "- Fit a label encoder to convert String labels into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "encoded_labels = le.transform(labels)\n",
    "n_classes = len(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Classifier\n",
    "- Split the data into training and testing sets\n",
    "- Train a linear SVM on the training data\n",
    "- Provide results of the accuracy on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=True, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, encoded_labels,test_size=0.25)\n",
    "clf = SVC(kernel='linear',probability=True)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       232\n",
      "           1       0.87      0.86      0.86       250\n",
      "           2       0.78      0.85      0.81       248\n",
      "           3       0.73      0.79      0.76       265\n",
      "           4       0.80      0.82      0.81       269\n",
      "           5       0.84      0.87      0.86       236\n",
      "           6       0.97      0.94      0.95       234\n",
      "           7       0.93      0.94      0.94       250\n",
      "           8       0.83      0.85      0.84       262\n",
      "           9       0.90      0.89      0.89       262\n",
      "          10       0.86      0.87      0.87       238\n",
      "          11       0.94      0.91      0.92       261\n",
      "          12       0.93      0.93      0.93       249\n",
      "          13       0.93      0.90      0.91       242\n",
      "          14       0.86      0.82      0.84       252\n",
      "          15       0.93      0.93      0.93       256\n",
      "          16       0.90      0.91      0.91       231\n",
      "          17       0.89      0.86      0.87       253\n",
      "          18       0.90      0.87      0.88       260\n",
      "\n",
      "    accuracy                           0.88      4750\n",
      "   macro avg       0.88      0.88      0.88      4750\n",
      "weighted avg       0.88      0.88      0.88      4750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can you test the shallow classifier on LDA features only?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "- Define a function to process a string to be suitable to run against the model\n",
    "- Use the built SVM model to predict the class\n",
    "- Output the top three classes with their probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_features(text,ldas,tf_vectorizers):\n",
    "        ''' extract lda features for a given text.\n",
    "        Args:\n",
    "            text: string\n",
    "            ldas: a list of LDA models\n",
    "            tf_vectorizers: a list of CounterVectorizers associated with the ldas\n",
    "        Returns:\n",
    "            a list of the lda features with length [number of lda models] X [number of topics per model] \n",
    "        '''\n",
    "        line= []\n",
    "        for j in range(len(ldas)):\n",
    "            line.extend(ldas[j].transform(tf_vectorizers[j].transform([text]))[0])\n",
    "        vec = nlp(text).vector\n",
    "        line.extend(list(vec))\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a business description please, q to exit:\n",
      "\n",
      "protection from financial lose, risk managment, risk of uncertainty \n",
      "financial : 34.90091462674359\n",
      "health care : 17.66488421582849\n",
      "Utilities : 8.665263563312925\n",
      "*****************\n",
      "\n",
      "Enter a business description please, q to exit:\n",
      "\n",
      "q\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print('Enter a business description please, q to exit:\\n')\n",
    "    st = input()\n",
    "    if st == 'q':\n",
    "        break\n",
    "    clean_st =  string_preprocessing(st)\n",
    "    feats = string_features(clean_st,ldas,tf_vectorizers)\n",
    "    probs = clf.predict_proba([feats])[0]\n",
    "    idx = np.argsort(probs)[::-1]\n",
    "    top_probs = probs[idx[:3]]\n",
    "    top_labels = le.inverse_transform(idx[:3])\n",
    "\n",
    "    for lbl,prob in zip(top_labels,top_probs):\n",
    "        print(lbl,':',100*prob)\n",
    "    print ('*****************\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "- Define a Multi-Layer Perceptron to classify the data\n",
    "- Use Two layer and a softmax layer \n",
    "- Use Dropout\n",
    "- Use Relu activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1150)              783150    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1150)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1150)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               575500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 19)                9519      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 19)                0         \n",
      "=================================================================\n",
      "Total params: 1,368,169\n",
      "Trainable params: 1,368,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "# First Layer\n",
    "model.add(Dense(1150, input_dim=len(X_train[0])))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#Second Layer\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "#Third Layer\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the labels from a  noiminal value to one-hot encoded\n",
    "this is a requirment to be able to run the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_label = to_categorical(y_train, num_classes=n_classes)\n",
    "test_label =  to_categorical(y_test, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "- Define a model checkpoint to save the best model through the training iterations\n",
    "- Define batch size and Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Noura/anaconda/envs/workshop/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 14250 samples, validate on 4750 samples\n",
      "Epoch 1/20\n",
      "14250/14250 [==============================] - 12s 814us/step - loss: 0.9406 - acc: 0.7448 - val_loss: 0.4981 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.85979, saving model to tutorial_weights.01-0.50.hdf5\n",
      "Epoch 2/20\n",
      "14250/14250 [==============================] - 8s 548us/step - loss: 0.4398 - acc: 0.8752 - val_loss: 0.4428 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.85979 to 0.87853, saving model to tutorial_weights.02-0.44.hdf5\n",
      "Epoch 3/20\n",
      "14250/14250 [==============================] - 8s 564us/step - loss: 0.3432 - acc: 0.8987 - val_loss: 0.4407 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.87853\n",
      "Epoch 4/20\n",
      "14250/14250 [==============================] - 8s 565us/step - loss: 0.2933 - acc: 0.9127 - val_loss: 0.4205 - val_acc: 0.8798\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.87853 to 0.87979, saving model to tutorial_weights.04-0.42.hdf5\n",
      "Epoch 5/20\n",
      "14250/14250 [==============================] - 8s 565us/step - loss: 0.2489 - acc: 0.9242 - val_loss: 0.4119 - val_acc: 0.8893\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.87979 to 0.88926, saving model to tutorial_weights.05-0.41.hdf5\n",
      "Epoch 6/20\n",
      "14250/14250 [==============================] - 8s 556us/step - loss: 0.2144 - acc: 0.9331 - val_loss: 0.4196 - val_acc: 0.8842\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.88926\n",
      "Epoch 7/20\n",
      "14250/14250 [==============================] - 8s 564us/step - loss: 0.1805 - acc: 0.9439 - val_loss: 0.4164 - val_acc: 0.8920\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.88926 to 0.89200, saving model to tutorial_weights.07-0.42.hdf5\n",
      "Epoch 8/20\n",
      "14250/14250 [==============================] - 8s 563us/step - loss: 0.1601 - acc: 0.9513 - val_loss: 0.4330 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.89200\n",
      "Epoch 9/20\n",
      "14250/14250 [==============================] - 8s 560us/step - loss: 0.1431 - acc: 0.9538 - val_loss: 0.4758 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.89200\n",
      "Epoch 10/20\n",
      "14250/14250 [==============================] - 8s 556us/step - loss: 0.1255 - acc: 0.9599 - val_loss: 0.4569 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.89200\n",
      "Epoch 11/20\n",
      "14250/14250 [==============================] - 8s 565us/step - loss: 0.1112 - acc: 0.9638 - val_loss: 0.4716 - val_acc: 0.8884\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.89200\n",
      "Epoch 12/20\n",
      "14250/14250 [==============================] - 8s 564us/step - loss: 0.0974 - acc: 0.9689 - val_loss: 0.4695 - val_acc: 0.8872\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.89200\n",
      "Epoch 13/20\n",
      "14250/14250 [==============================] - 8s 561us/step - loss: 0.0921 - acc: 0.9712 - val_loss: 0.5010 - val_acc: 0.8882\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89200\n",
      "Epoch 14/20\n",
      "14250/14250 [==============================] - 8s 558us/step - loss: 0.0783 - acc: 0.9745 - val_loss: 0.5021 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.89200\n",
      "Epoch 15/20\n",
      "14250/14250 [==============================] - 8s 565us/step - loss: 0.0782 - acc: 0.9740 - val_loss: 0.5300 - val_acc: 0.8869\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89200\n",
      "Epoch 16/20\n",
      "14250/14250 [==============================] - 8s 562us/step - loss: 0.0789 - acc: 0.9729 - val_loss: 0.5333 - val_acc: 0.8819\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89200\n",
      "Epoch 17/20\n",
      "14250/14250 [==============================] - 8s 559us/step - loss: 0.0703 - acc: 0.9749 - val_loss: 0.5545 - val_acc: 0.8903\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.89200\n",
      "Epoch 18/20\n",
      "14250/14250 [==============================] - 8s 561us/step - loss: 0.0581 - acc: 0.9808 - val_loss: 0.5832 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.89200\n",
      "Epoch 19/20\n",
      "14250/14250 [==============================] - 8s 565us/step - loss: 0.0617 - acc: 0.9792 - val_loss: 0.5693 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.89200\n",
      "Epoch 20/20\n",
      "14250/14250 [==============================] - 8s 566us/step - loss: 0.0566 - acc: 0.9808 - val_loss: 0.5748 - val_acc: 0.8926\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.89200 to 0.89263, saving model to tutorial_weights.20-0.57.hdf5\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='tutorial_weights.{epoch:02d}-{val_loss:.2f}.hdf5', verbose=1, save_best_only=True,monitor='val_acc')\n",
    "hist=model.fit(np.asarray(X_train), train_label, epochs=20, batch_size=100,\n",
    "               validation_data=(np.asarray(X_test),test_label),callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the output to understand the change of training and validation accuracy over training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a759f0f90>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       232\n",
      "           1       0.87      0.87      0.87       250\n",
      "           2       0.83      0.85      0.84       248\n",
      "           3       0.86      0.80      0.83       265\n",
      "           4       0.81      0.83      0.82       269\n",
      "           5       0.87      0.92      0.89       236\n",
      "           6       0.98      0.93      0.95       234\n",
      "           7       0.95      0.92      0.94       250\n",
      "           8       0.82      0.89      0.86       262\n",
      "           9       0.90      0.92      0.91       262\n",
      "          10       0.92      0.87      0.89       238\n",
      "          11       0.96      0.93      0.94       261\n",
      "          12       0.93      0.93      0.93       249\n",
      "          13       0.87      0.94      0.90       242\n",
      "          14       0.86      0.84      0.85       252\n",
      "          15       0.93      0.93      0.93       256\n",
      "          16       0.93      0.93      0.93       231\n",
      "          17       0.92      0.84      0.88       253\n",
      "          18       0.86      0.90      0.88       260\n",
      "\n",
      "    accuracy                           0.89      4750\n",
      "   macro avg       0.89      0.89      0.89      4750\n",
      "weighted avg       0.89      0.89      0.89      4750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(model.predict(np.asarray(X_test)),axis=1)\n",
    "print(metrics.classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you test different variations of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data to use in LSTM\n",
    "- For LSTM the data has to be prepared differently to format it as sequences\n",
    "- Each element in the sequence is a word embedding from the spaCy language model\n",
    "- Define max sequence length\n",
    "- Add zero paddings for shorter seuquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding,LSTM,Bidirectional\n",
    "from keras.models import Model,Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = nlp.vocab.length # the number of words in the dictionary\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "EMBEDDING_DIM = 300 # comes from spaCy, if you select a different model this has to change accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare text samples and their labels\n",
    "texts = df['Doc'].apply(lambda x: string_preprocessing(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Keras tokenizer and convert text into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21903 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add padding if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_seq = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19000, 10)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embedding layer which is not trainable, this will be the input to the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = nlp(word).vector\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 300)           6571200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                6030      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 19)                589       \n",
      "=================================================================\n",
      "Total params: 6,978,619\n",
      "Trainable params: 407,419\n",
      "Non-trainable params: 6,571,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "batch_size = 100\n",
    "print('Build model...')\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(LSTM(200, dropout=0.3, recurrent_dropout=0.3,return_sequences=False)) \n",
    "# if you want to add another layer set return_sequences to True\n",
    "#lstm_model.add(Bidirectional(LSTM(50, dropout=0.5, recurrent_dropout=0.5)))\n",
    "lstm_model.add(Dense(30, activation='tanh'))\n",
    "lstm_model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "lstm_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the labels\n",
    "seq_labels = to_categorical(encoded_labels,num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15200 samples, validate on 3800 samples\n",
      "Epoch 1/20\n",
      "15200/15200 [==============================] - 23s 2ms/step - loss: 1.4893 - acc: 0.5959 - val_loss: 4.6712 - val_acc: 0.0371\n",
      "Epoch 2/20\n",
      "15200/15200 [==============================] - 18s 1ms/step - loss: 0.9874 - acc: 0.7255 - val_loss: 5.3133 - val_acc: 0.0571\n",
      "Epoch 3/20\n",
      "15200/15200 [==============================] - 18s 1ms/step - loss: 0.8803 - acc: 0.7511 - val_loss: 5.3847 - val_acc: 0.0782\n",
      "Epoch 4/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.8001 - acc: 0.7719 - val_loss: 5.7765 - val_acc: 0.1053\n",
      "Epoch 5/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.7439 - acc: 0.7845 - val_loss: 5.8496 - val_acc: 0.1037\n",
      "Epoch 6/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.7022 - acc: 0.7952 - val_loss: 6.1645 - val_acc: 0.0968\n",
      "Epoch 7/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.6418 - acc: 0.8130 - val_loss: 6.1563 - val_acc: 0.1139\n",
      "Epoch 8/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.5978 - acc: 0.8246 - val_loss: 6.3142 - val_acc: 0.0945\n",
      "Epoch 9/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.5529 - acc: 0.8406 - val_loss: 6.4506 - val_acc: 0.1071\n",
      "Epoch 10/20\n",
      "15200/15200 [==============================] - 18s 1ms/step - loss: 0.5124 - acc: 0.8499 - val_loss: 6.5000 - val_acc: 0.1150\n",
      "Epoch 11/20\n",
      "15200/15200 [==============================] - 18s 1ms/step - loss: 0.4673 - acc: 0.8611 - val_loss: 6.6400 - val_acc: 0.1147\n",
      "Epoch 12/20\n",
      "15200/15200 [==============================] - 18s 1ms/step - loss: 0.4298 - acc: 0.8701 - val_loss: 6.4230 - val_acc: 0.1134\n",
      "Epoch 13/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.3823 - acc: 0.8882 - val_loss: 6.8102 - val_acc: 0.1126\n",
      "Epoch 14/20\n",
      "15200/15200 [==============================] - 18s 1ms/step - loss: 0.3459 - acc: 0.8993 - val_loss: 6.9566 - val_acc: 0.1055\n",
      "Epoch 15/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.3125 - acc: 0.9095 - val_loss: 6.8156 - val_acc: 0.1161\n",
      "Epoch 16/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.2784 - acc: 0.9191 - val_loss: 6.9721 - val_acc: 0.1155\n",
      "Epoch 17/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.2479 - acc: 0.9266 - val_loss: 7.5719 - val_acc: 0.1208\n",
      "Epoch 18/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.2140 - acc: 0.9387 - val_loss: 7.3249 - val_acc: 0.1179\n",
      "Epoch 19/20\n",
      "15200/15200 [==============================] - 17s 1ms/step - loss: 0.1940 - acc: 0.9436 - val_loss: 7.6297 - val_acc: 0.1266\n",
      "Epoch 20/20\n",
      "15200/15200 [==============================] - 20s 1ms/step - loss: 0.1753 - acc: 0.9495 - val_loss: 7.5099 - val_acc: 0.1250\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (680,) but got array with shape (10,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-5fa273a5a887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m score, acc = model.evaluate(data_seq, seq_labels,\n\u001b[0;32m----> 9\u001b[0;31m                             batch_size=batch_size)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/workshop/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/workshop/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/workshop/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (680,) but got array with shape (10,)"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='weights-improvement-lstm-{epoch:02d}-{val_acc:.2f}.hdf5',\n",
    "                                            monitor='val_loss', verbose=0, save_best_only=True)\n",
    "hist = lstm_model.fit(data_seq, seq_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,validation_split=0.2,shuffle=True,callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "score, acc = model.evaluate(data_seq, seq_labels,\n",
    "                            batch_size=batch_size)\n",
    "print('Train score:', score)\n",
    "print('Train accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lstm_model.predict_classes(data_seq)\n",
    "print(classification_report(np.argmax(seq_labels,axis=1),pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is that a better model than an MLP?\n",
    "- if not what can you change?\n",
    "- Is the test correct?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
