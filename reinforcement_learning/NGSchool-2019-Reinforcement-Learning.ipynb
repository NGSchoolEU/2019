{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGSchool2019\n",
    "## Reinforcement Learning Tutorial\n",
    "\n",
    "### Adapted from a tutorial by [Katja Hofmann](https://www.microsoft.com/en-us/research/people/kahofman/)\n",
    "\n",
    "### Presented by Robert Loftin\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "This tutoria demonstrates how to build a deep RL agent that learns to navigate: first in SimpleRooms, a task we implement from scratch, then in a MineRL task in Minecraft. \n",
    "\n",
    "In this tutorial we will learn how to implement a deep reinforcement learning agent, namely, Deep Q-Networks (DQN) [Mnih et al. 2015](https://www.nature.com/articles/nature14236/), and train it to navigate in a simple 2D environment.  In an optional section at the end of this tutorial, you will have the opportunity to train your deep RL agent in the [MineRL](http://minerl.io/) environment, built around the popular video game [Minecraft](https://www.minecraft.net/en-us/).\n",
    "\n",
    "1. [Setup](#Setup) **Tip: run this section before the start of the tutorial, to make sure you're ready to get started.**\n",
    "1. [RL Components](#RL-Components): **Learn how to implement the core components of an RL experiment: environment, agent, and the experiment itself.**\n",
    "  1. [Agent](#Agent)\n",
    "  1. [Experiment](#Experiment)\n",
    "  1. [Experiment 1: Random Agent on MountainCar](#Experiment-1:-Random-Agent-on-MountainCar)\n",
    "  1. [Environment: Four Rooms](#Environment:-Four-Rooms)\n",
    "  1. [Experiment 2: Random Agent on SimpleRooms](#Experiment-2:-Random-Agent-on-SimpleRooms)\n",
    "\n",
    "1. [DQN Agent Implementation](#DQN-Agent-Implementation): **Learn how to implement a DQN Agent**\n",
    "  1. [QNetwork](#QNetwork)\n",
    "  1. [Replay Memory](#Replay-Memory)\n",
    "  1. [Exploration](#Exploration)\n",
    "  1. [QLearning Agent](#QLearning-Agent)\n",
    "1. [Experiment 3: Deep Q-Networks on FourRooms](#Experiment-3:-Deep-Q-Networks-on-FourRooms): **Experiment with a DQN Agent**\n",
    "\n",
    "1. [Further Reading](#Further-Reading)\n",
    "1. [(Optional) DQN on MineRL](#(Optional)-DQN-on-MineRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before we start, you will need to make sure that all of the python dependencies needed for this tutorial are installed and working properly.  If possible, I recommend running these sections before the tutorial session, so you will have your evironment ready to go when we start.\n",
    "\n",
    "### Install requirments\n",
    "\n",
    "Install all of the dependencies for this tutorial in you python environment.  You won't need to run this again once everything is installed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install --upgrade gym matplotlib==3.0.3 numpy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "\n",
    "Import all of the packages we will need for this tutorial.  This needs to be run whenever you reload this notebook or restart the Ipython kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells the notebook to enable its matplotlib backend so we can draw stuff\n",
    "%matplotlib nbagg\n",
    "\n",
    "# Dependencies for visualizing environments and ploting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Dependencies for our environments and learning algorithms\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Components\n",
    "\n",
    "In this section we will build some of the elements we will need to run and visualize the RL experiments in this notebook.  The main components will be:\n",
    "\n",
    "**Agent:** interacts with an environment by taking actions and receiving observations and rewards. To start, we will just define the abstract template for an RL agent, and implement an agent which just takes random actions.  Once we have everything set up, we will implement some actual deep RL agents, including a Deep Q-Network agent, and a Proximal Policy Optimization agent.\n",
    "\n",
    "**Environment:** defines an interactive task. Our environments will implement the [OpenAI gym interface](https://gym.openai.com/).  We will start with two classic RL environments, CartPole and MountainCar, which are already implemented as prt of the gym.  We will also implement a simple custom environment to illustrate gym functionality.  \n",
    "\n",
    "In the optional section at the end of the tutorial we look at MineRL, a much more complex 3D environment based on popular video game [Minecraft](https://www.minecraft.net/en-us/).\n",
    "\n",
    "**Experiment:** connects our agents to our environments.  This class will allow us to easily run and visualize our reinforcement learning experiements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "Run the cell below to define the abstract agent class, and an implementation of this class which samples actions uniformly at random.  Notice that agents will only implement a single method, step(), which takes in the most recent observation and reward, and outputs the next action for the agent to take.  Also notice that this method takes a parameter 'done', which indicates the end of an episode.\n",
    "\n",
    "**Note:** The 'info' parameter is part of the OpenAI gym interface, and allows an environment to return additional information beyond the current observation.  We won't use this here, but include it for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    '''Agent base class'''\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def step(self, obs, reward, done, info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    '''Agent that samples actions uniformly at random'''\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(RandomAgent, self).__init__(observation_space, action_space)\n",
    "    \n",
    "    def step(self, obs, reward, done, info):\n",
    "        '''Sample a random action from the action space.  Gym defines anaction space \n",
    "        interface, which implements the sample() method for generating random actions'''\n",
    "        \n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "Run the cells below to define the Experiment class, which allows us to configure, run, and visualize our RL experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "\n",
    "    def __init__(self, env, agent, window_size=100, r_min=0.0, r_max=0.0):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "\n",
    "        # Since the reward signal may have high variance, we plot a rolling average\n",
    "        self.rolling_average = np.array([0.0])\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.r_min = r_min\n",
    "        self.r_max = r_max\n",
    "        \n",
    "        # prepare visuals\n",
    "        self.fig = plt.figure(figsize=(9, 5))\n",
    "        gs = gridspec.GridSpec(1, 2)\n",
    "        env_fig = plt.subplot(gs[0, 0])\n",
    "        env_fig.title.set_text('Environment Visualization')\n",
    "        env_fig.xaxis.set_visible(False)\n",
    "        env_fig.yaxis.set_visible(False)\n",
    "        self.env_img = env_fig.imshow(np.random.random((64,64)), interpolation='none', cmap='viridis')\n",
    "        \n",
    "        self.reward_fig = plt.subplot(gs[0, 1])\n",
    "        self.reward_fig.title.set_text('Rolling average reward')\n",
    "        self.reward_line, = self.reward_fig.plot(range(len(self.rolling_average)), self.rolling_average)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def run(self, num_steps, display_frequency=1):\n",
    "        observation = self.env.reset()  # Initialize environment\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        info = None\n",
    "        steps = 0\n",
    "        rewards = np.array([])\n",
    "        self.reward_fig.set_xlim(0, max(100, num_steps))\n",
    "        self.update_display()\n",
    "        \n",
    "        while steps < num_steps:\n",
    "            \n",
    "            # if the episode is finished, reset the environment\n",
    "            if done:\n",
    "                observation = self.env.reset() \n",
    "            \n",
    "            steps += 1\n",
    "            \n",
    "            action = self.agent.step(observation, reward, done, info)  # Get the agent's next action\n",
    "            observation, reward, done, info = self.env.step(action)  # Take a step in the environment\n",
    "                \n",
    "            # Update the rolling average reward\n",
    "            rewards = np.append(rewards, reward)\n",
    "            self.rolling_average = np.append(self.rolling_average, np.mean(rewards[-self.window_size:]))\n",
    "\n",
    "            # Update the visualization\n",
    "            if steps % display_frequency == 0:\n",
    "                self.update_display()\n",
    "      \n",
    "    def update_display(self):\n",
    "        \n",
    "        # Draw the environment\n",
    "        self.env_img.set_data(self.env.render(mode='rgb_array'))\n",
    "        \n",
    "        # Plot the average reward\n",
    "        self.reward_line.set_data(range(len(self.rolling_average)), self.rolling_average)\n",
    "        self.reward_fig.set_ylim(min(self.r_min, min(self.rolling_average)-0.1), \n",
    "                                 max(self.r_max, max(self.rolling_average)+0.1))\n",
    "\n",
    "        self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Random Agent on MountainCar\n",
    "\n",
    "We are now ready to run our fist simple experiment.  This will just illustrate the proces of conecting an agent to an environment and running an experiment.  Before we build own custom environment, we will use a classic RL environment, MountainCar, which is already implemented in gym.  Run the cells below to launch our first experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment setup\n",
    "env = gym.make('MountainCar-v0')\n",
    "random_agent = RandomAgent(env.observation_space, env.action_space)\n",
    "experiment = Experiment(env, random_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "experiment.run(num_steps=200, display_frequency=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have just run your first RL experiment.  Of course, your agent hasn't actually learned anything yet (it can't make it to the top of the mountain) but this is how you will run all of the experiments in this tutorial.  Take a step back to familiarize yourself with the code up to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment: Four Rooms\n",
    "\n",
    "Before we get to the actual RL algorithms, we will first try implementing a simple RL environment of our own.  This will give us a chance to go through the process of turning decision-making problem we want to solve into an RL environment which we can actually train in.\n",
    "\n",
    "Our environment will be a 4x4 grid world, divided by walls into four different rooms.  The agent can move up, down, left or right, but cannot move through walls.  The objective for the agent is to reach a random goal location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourRooms(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (16,))\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.rewards = np.zeros(16)\n",
    "        self.rewards[np.random.randint(0, 16)] = 100.0\n",
    "        \n",
    "        self.state = 0\n",
    "        self.num_steps = 0\n",
    "        \n",
    "        self.P = [None] * 16\n",
    "        self.P[0] = [0, 4, 0, 1]\n",
    "        self.P[1] = [1, 5, 0, 2]\n",
    "        self.P[2] = [2, 6, 1, 3]\n",
    "        self.P[3] = [3, 7, 2, 3]\n",
    "        self.P[4] = [0, 8, 4, 5]\n",
    "        self.P[5] = [1, 5, 4, 5]\n",
    "        self.P[6] = [2, 6, 6, 7]\n",
    "        self.P[7] = [3, 11, 6, 7]\n",
    "        self.P[8] = [4, 12, 8, 9]\n",
    "        self.P[9] = [9, 13, 8, 9]\n",
    "        self.P[10] = [10, 14, 10, 11]\n",
    "        self.P[11] = [7, 15, 10, 11]\n",
    "        self.P[12] = [8, 12, 12, 13]\n",
    "        self.P[13] = [9, 13, 12, 14]\n",
    "        self.P[14] = [10, 14, 13, 15]\n",
    "        self.P[15] = [11, 15, 14, 15]\n",
    "\n",
    "        self.max_episode_length = 40  # End episode automatically after 20 steps\n",
    "        self._background = self._render_maze()  # The maze and goal are fixed, so just render them once\n",
    "\n",
    "    def step(self, action):\n",
    "        if action < 0 or action > 3:\n",
    "            raise ValueError('Unknown action', action)\n",
    "        \n",
    "        self.state = self.P[self.state][action]\n",
    "        self.num_steps += 1\n",
    "        \n",
    "        obs = self._one_hot(self.state)\n",
    "        reward = self.rewards[self.state]\n",
    "        done = (reward != 0.0) or (self.num_steps >= self.max_episode_length)\n",
    "        \n",
    "        return obs, reward, done, None\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.randint(16)\n",
    "        self.num_steps = 0\n",
    "        \n",
    "        # Don't spawn in a reward state\n",
    "        while self.rewards[self.state] != 0.0:\n",
    "            self.state = np.random.randint(16)\n",
    "\n",
    "        return self._one_hot(self.state)\n",
    "\n",
    "    def _one_hot(self, state):\n",
    "        obs = np.zeros(16, dtype=np.float32)\n",
    "        obs[state] = 1.0\n",
    "        return obs\n",
    "    \n",
    "    def _render_coords(self, s):\n",
    "        return ((s % 4) * 4, int(s / 4) * 4)\n",
    "\n",
    "    def _render_maze(self):\n",
    "        maze = np.zeros((17, 17))\n",
    "        for x in range(0, 17, 4):\n",
    "            maze[x, :] = .2\n",
    "        for y in range(0, 17, 4):\n",
    "            maze[:, y] = .2\n",
    "\n",
    "        for s in range(16):\n",
    "            x, y = self._render_coords(s)\n",
    "            if self.P[s][0] == s:\n",
    "                maze[x:x+5, y] = .5\n",
    "            if self.P[s][1] == s:\n",
    "                maze[x:x+5, y+4] = .5\n",
    "            if self.P[s][2] == s:\n",
    "                maze[x, y:y+5] = .5\n",
    "            if self.P[s][3] == s:\n",
    "                maze[x+4, y:y+5] = .5\n",
    "            if self.rewards[s] != 0:\n",
    "                maze[x+1:x+4, y+1:y+4] = self.rewards[s]\n",
    "        return maze\n",
    "\n",
    "    def render(self, mode='rgb_array'):\n",
    "        assert mode == 'rgb_array', 'Unknown mode: %s' % mode\n",
    "        img = np.array(self._background, copy=True)\n",
    "        x, y = self._render_coords(self.state)\n",
    "        img[x+1:x+4, y+1:y+4] = .8\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Random Agent on FourRooms\n",
    "\n",
    "We can now run the random agent in our new environment.  Run the cell below to set up a new experiment with the FourRooms environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAgent(Agent):\n",
    "    '''An agent that follows a fixed policy'''\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(RandomAgent, self).__init__(observation_space, action_space)\n",
    "        self._policy = [0] * observation_space.n\n",
    "    \n",
    "        # TODO: create a policy that will reach a fixed goal\n",
    "    \n",
    "    def step(self, obs, reward, done, info):\n",
    "        for s in range(len(obs)):\n",
    "            if obs[s] > 0.0:\n",
    "                return self._policy[s]\n",
    "        \n",
    "        return self.action_space.sample()\n",
    "\n",
    "env = FourRooms()\n",
    "random_agent = RandomAgent(env.observation_space, env.action_space)\n",
    "experiment = Experiment(env, random_agent, window_size=100, r_max=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run(num_steps=300, display_frequency=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Consider the implementation of the FourRooms environment.  For RL to work, this environment must correspond to a Markov Decision Process (MDP).  Can you identify the key elements of an MDP here?  What are the state and action spaces for this environment?  How many states are there? What are the transition probabilities? What is reward function?\n",
    "\n",
    "**Exercise 2:** Now modify the environment so the goal location is fixed instead of random.  Take a look at the PolicyAgent class above.  Can you define a policy which allows this agent to reach the fixed goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent Implementation\n",
    "\n",
    "At long last, we are finally ready to implement an actual reinforcement learning agent. In this section, we will implement a Deep Q-Network (DQN) agent (see [Mnih et al. 2015](https://www.nature.com/articles/nature14236/) for details).  Our implementation will have four components:\n",
    "\n",
    "**Q-Network:** the deep network representing the state-action value function (the Q-function).\n",
    "\n",
    "**Replay Buffer:** stores the agents experience as (state, action, reward, next state) tuples.\n",
    "\n",
    "**Exploration Strategy:** selects action for the agent to take based on the predicted Q-values for those actions.\n",
    "\n",
    "**DQN Agent:** the agent itself, which selects actions and performs the actual learning update\n",
    "\n",
    "These components are implemented in turn in the cells below.\n",
    "\n",
    "**Note:** some of these implementations are incomplete - you will need to fill in the missing pieces in the exercises below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Network\n",
    "\n",
    "Here we represent our Q-function with a fully connected, 2 layer network implemented in PyTorch.  \n",
    "\n",
    "**Exercise 3:** You do not need to add anything to this class, but you should take a moment to familiarize yourself with the code. Notice that the network does not take an action as input.  How do we get the Q-value prediction for a specific action?  How would we represent the Q-function for a continuous action space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_size, num_actions, num_hidden):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        hidden = nn.Linear(obs_size, num_hidden)\n",
    "        nn.init.kaiming_uniform_(hidden.weight.data, nonlinearity=\"relu\")\n",
    "        nn.init.uniform_(hidden.bias.data, a=-1e-4, b=1e-4)\n",
    "        \n",
    "        output = nn.Linear(num_hidden, num_actions)\n",
    "        nn.init.kaiming_uniform_(output.weight.data, nonlinearity=\"relu\")\n",
    "        nn.init.uniform_(output.bias.data, a=-1e-4, b=1e-4)\n",
    "        \n",
    "        self._layers = nn.Sequential(hidden, nn.ReLU(), output)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self._layers(torch.as_tensor(obs, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "This class stores a collection of recent experiences, and samples mini-batches of experiences to train the Q-Network on.  You do not need to add anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"Implements basic replay memory\"\"\"\n",
    "\n",
    "    def __init__(self, obs_size, max_size=1000):\n",
    "        self._max_size = max_size\n",
    "        self._num_observed = 0\n",
    "        \n",
    "        self.samples = {\n",
    "            'obs': np.zeros((max_size, obs_size), dtype=np.float32),\n",
    "            'action': np.zeros(max_size, dtype=np.int64),\n",
    "            'reward': np.zeros(max_size, dtype=np.float32),\n",
    "            'done': np.zeros(max_size, dtype=np.float32)\n",
    "        }\n",
    "    \n",
    "    def observe(self, obs, action, reward, done):\n",
    "        index = self._num_observed % self._max_size\n",
    "        self._num_observed += 1\n",
    "        \n",
    "        self.samples['obs'][index, :] = np.array(obs, copy=False)\n",
    "        self.samples['action'][index] = action\n",
    "        self.samples['reward'][index] = reward\n",
    "        self.samples['done'][index] = 1.0 if done else 0.0\n",
    "    \n",
    "    def sample_minibatch(self, minibatch_size):\n",
    "        indices = np.random.randint(min(self._num_observed, self._max_size) - 1, size=minibatch_size)\n",
    "        obs = torch.as_tensor(self.samples['obs'][indices, :])\n",
    "        actions = torch.as_tensor(self.samples['action'][indices])\n",
    "        rewards = torch.as_tensor(self.samples['reward'][indices])\n",
    "        done = torch.as_tensor(self.samples['done'][indices])\n",
    "        next_obs = torch.as_tensor(self.samples['obs'][indices + 1, :])\n",
    "\n",
    "        return obs, actions, rewards, done, next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration Strategy\n",
    "\n",
    "These classes define different exploration strategies the DQN agent could use.  Exploration is an essential part of reinforcement learning.  Without trying new actions, the agent may miss opportunities to learn more about its environment and improve its policy.\n",
    "\n",
    "**Exercise 4:** The cell below defines two classes, each describing a different exploration strategy, but they are incomplete.  Implement the epsilon-greedy exploration strategy.  (Optional) Implement the softmax exploration strategy.\n",
    "\n",
    "**Hint:** Each exploration strategy just takes in the Q-values for the current state, and outputs an action for the agent to actually perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyExplorer(object):\n",
    "    \"\"\"Implements an epsilon greedy exploration strategy\"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def next_action(self, numpy_q_values_for_state):\n",
    "\n",
    "        # TODO: implement epsilon-greedy exploration - see Exercise 4\n",
    "\n",
    "        return action_index\n",
    "    \n",
    "\n",
    "class SoftmaxExplorer(object):\n",
    "    \"\"\"Implements a softmax exploration strategy\"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions, beta=1.0):\n",
    "        self.beta = beta\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def next_action(self, numpy_q_values_for_state):\n",
    "        \n",
    "        # TODO (Optional): implement softmax exploration - see Exercise 4\n",
    "\n",
    "        return action_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearning Agent\n",
    "\n",
    "Finally, we implement the DQN agent itself, using the Q-network, replay buffer, and exploration strategies we have defined above.  This implementation is not quite complete however.\n",
    "\n",
    "**Exercise 5:**  Familiarize yourself with the QLearningAgent class.  Notics that there is a TODO comment in the update_model function.  Here you will need to add the code to compute the targets Q-values that the model will need to learn.\n",
    "\n",
    "**Hint:**  You should be able to do this with one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    \"\"\"Q-Learning agent with function approximation.\"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, **kwargs):\n",
    "        super(QLearningAgent, self).__init__(observation_space, action_space)\n",
    "\n",
    "        obs_size = observation_space.shape[0]\n",
    "        num_actions = action_space.n\n",
    "        \n",
    "        self.model_network = QNetwork(obs_size, num_actions, kwargs.get('num_hidden', 128))\n",
    "        self.target_network = QNetwork(obs_size, num_actions, kwargs.get('num_hidden', 128))\n",
    "        self.target_network.load_state_dict(self.model_network.state_dict())\n",
    "    \n",
    "        self.explorer = kwargs.get('explorer', EpsilonGreedyExplorer(action_space.n, 0.1))\n",
    "        self.memory = ReplayMemory(obs_size, kwargs.get('memory_size', 5000))\n",
    "        self.optimizer = torch.optim.Adam(self.model_network.parameters(), kwargs.get('learning_rate', 0.01))\n",
    "\n",
    "        self.gamma = kwargs.get('gamma', .99)\n",
    "        self.minibatch_size = kwargs.get('minibatch_size', 32)\n",
    "        self.epoch_length = kwargs.get('epoch_length', 100)\n",
    "        \n",
    "        self.num_steps = 0\n",
    "        self.prev_action = None\n",
    "        self.prev_obs = None\n",
    "        \n",
    "    def step(self, obs, reward, done, info):\n",
    "        if self.num_steps > 0:\n",
    "            self.memory.observe(self.prev_obs, self.prev_action, reward, done)\n",
    "\n",
    "        action = self.explorer.next_action(self.model_network(obs).detach().numpy())\n",
    "\n",
    "        # start training after 1 epoch\n",
    "        if self.num_steps > self.epoch_length:\n",
    "            self.update_model()\n",
    "\n",
    "        self.num_steps += 1\n",
    "        self.prev_action = action\n",
    "        self.prev_obs = obs\n",
    "\n",
    "        if self.num_steps % self.epoch_length == 0:\n",
    "            self.target_network.load_state_dict(self.model_network.state_dict()) # Update target network\n",
    "            \n",
    "            # If epsilon greedy - decay epsilon after each epoch\n",
    "            if isinstance(self.explorer, EpsilonGreedyExplorer):\n",
    "                self.explorer.epsilon = max(0.05, self.explorer.epsilon * .95)\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update_model(self):\n",
    "        # sample minibatch\n",
    "        obs, actions, rewards, dones, next_obs = self.memory.sample_minibatch(self.minibatch_size)\n",
    "        \n",
    "        # TODO: implement Q-target computation (max_a' Q_hat(s_next, a')), see Exercise 5\n",
    "        # Q_targets = None\n",
    "\n",
    "        # Get the current Q-values\n",
    "        Q_values = self.model_network(obs)\n",
    "        Q_values = torch.gather(Q_values, 1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # compute Huber loss - a smoothed L1 loss\n",
    "        loss = torch.nn.functional.smooth_l1_loss(Q_values, Q_targets.detach())\n",
    "\n",
    "        # perform model update\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 - Deep Q-Networks on FourRooms\n",
    "\n",
    "Now run the cells below to launch an experiment with DQN in our 'four rooms' environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FourRooms()\n",
    "q_agent = QLearningAgent(env.observation_space, env.action_space)\n",
    "experiment = Experiment(env, q_agent, window_size=100, r_max=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run(5000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!  If you have reached this point in the tutorial, you will have successfully implemented and run your first experiment with deep RL.\n",
    "\n",
    "**Exercise 6:** As you may have noticed, our DQN agent has a large number of configuration options (these are called **hyperparameters** in machine learning). These include the learning rate, the discount factor gamma, the number hidden nodes in the Q-network, and the choice of exploration strategy.  \n",
    "\n",
    "Try changing some of these values and running the experiment again.  Which values have the biggest impact on performance and what are the best values for these parameters?  How do you compare one set of hyperparameters against another, what is the performance measure?\n",
    "\n",
    "**(Optional) Exercise 7:** Try using your DQN agent to solve the 'MountainCar-v0' task we saw earlier in the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "Congratulations! You have completed this RL tutorial.\n",
    "\n",
    "Hopefully, this tutorial has piqued your interest in Reinforcement learning. Here are a few more resources to help you get started.\n",
    "\n",
    "**The RL Book:** For an in-depth treatment of RL, I highly recommend the Sutton and Barto book, now in its second edition: http://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    "**Code:** In this tutorial we implemented a DQN agent from scratch. A wide range of RL baselines and state-of-the-art algorithms are implemented in [chainerrl](https://github.com/chainer/chainerrl). Other popular RL implementations include OpenAI's [Spinning Up RL](https://spinningup.openai.com/en/latest/), the ray project's [RLLib](https://ray.readthedocs.io/en/latest/rllib.html), and Google's [Dopamine](https://github.com/google/dopamine).\n",
    "\n",
    "**Conferences:**\n",
    "For recent research in reinforcement learning, check out the topics discussed at [RLDM](http://rldm.org/) - an interdisciplinary conference on Reinforcement Learning and Decision Making. Other key conferences with a large portion of RL research are [ICML](https://www.icml.cc/), [ICLR](https://iclr.cc) and [NeurIPS](https://neurips.cc/). A popular event in Europe is the European Workshop on Reinforcement Learning [EWRL](https://ewrl.wordpress.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) DQN on MineRL\n",
    "\n",
    "Deep reinforcement learning has been able to solve extremely difficult control problems, including learning to play modern video games.  In this section, we will apply our DQN agent to the [MineRL](http://minerl.io/) environment, built around the popular video game [Minecraft](https://www.minecraft.net/en-us/).  MineRL was developed by a team led by [William H. Guss](http://wguss.ml/) and [Brandon Houghton](https://github.com/brandonhoughton) for the NeurIPS 2019 MineRL competition, hosted by AICrowd and sponsored by Microsoft. MineRL is based on [Project Malmo](https://www.microsoft.com/en-us/research/project/project-malmo/), developed at [Microsoft Research](https://www.microsoft.com/en-us/research/theme/game-intelligence/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install MineRL and Dependencies\n",
    "\n",
    "Install the MineRL package.  More detailed, platform-specific installation instructions can be found at: http://minerl.io/docs/tutorials/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade minerl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the MineRL package and the OpenCV dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environments\n",
    "import minerl\n",
    "import cv2\n",
    "\n",
    "# Uncomment in case Minecraft fails to start, to help with debugging:\n",
    "#import sys\n",
    "#import logging\n",
    "#logger = logging.getLogger(\"minerl\")\n",
    "#logger.setLevel(logging.DEBUG)\n",
    "#logger.addHandler(logging.StreamHandler(sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test that you can create a MineRL environment and interact with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Minecraft and create a MineRL environment - be patient, this will take several minutes\n",
    "minerl_env = gym.make('MineRLNavigateDense-v0')\n",
    "\n",
    "# If you turned on debugging above, quieten things down by uncommenting the below:\n",
    "#logger.setLevel(logging.INFO)\n",
    "\n",
    "# Test that you can reset and interact with the MineRL environment\n",
    "minerl_env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = minerl_env.action_space.sample()\n",
    "    minerl_env.step(action)\n",
    "    \n",
    "minerl_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the MineRL Environemnt\n",
    "\n",
    "Training an RL agent on the full MineRL environment would take hours, so we will simplify things to make training feasible within a few minutes.  Run the cells below to build a discrete action wrapper for the MineRL navigation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMinecraftNavigation(gym.Env):\n",
    "    '''Wrap the MineRL navigation environment to discretize actions and simplify observations'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('MineRLNavigateDense-v0')\n",
    "        self.observation_space = gym.spaces.Box(1.0, 1.0, (28,))\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.steps_this_episode = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs, _ = self.env.reset()\n",
    "        self.steps_this_episode = 0\n",
    "        return self._convert_obs(self.obs)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps_this_episode += 1\n",
    "        self.obs, self.reward, self.done, self.info = self.env.step(self._convert_action(action))\n",
    "        \n",
    "        # Make the reward signal more dense\n",
    "        if action == 0:\n",
    "            if obs['compassAngle'] < 1:\n",
    "                self.reward = .5\n",
    "            else:\n",
    "                self.reward = .1\n",
    "        else:\n",
    "            self.reward = -.3\n",
    "        \n",
    "        return self._convert_obs(self.obs), self.reward, self.done, self.info\n",
    "\n",
    "    def _convert_obs(self, obs):\n",
    "        # constructs obs of size 3 x 3 x 3 + 1 = 28\n",
    "        low_res = cv2.resize(obs['pov'], dsize=(3, 3), interpolation=cv2.INTER_NEAREST)\n",
    "        return np.float32(np.hstack([low_res.flatten(), obs['compassAngle']]))\n",
    "\n",
    "    def _convert_action(self, action):\n",
    "        base_action =  self.env.action_space.noop()\n",
    "        base_action['jump'] = 1\n",
    "        base_action['attack'] = 1\n",
    "\n",
    "        if action == 0:\n",
    "            # move forward\n",
    "            base_action['forward'] = 1\n",
    "        elif action == 1:\n",
    "            # turn towards the compass direction\n",
    "            base_action['camera'] = [0, 0.03 * obs['compassAngle']]\n",
    "        elif action == 2:\n",
    "            # move back\n",
    "            base_action['back'] = 1\n",
    "        else:\n",
    "            raise NotImplementedError('Action %d is not implemented.' % action)\n",
    "\n",
    "        return base_action\n",
    "\n",
    "    def render(self, mode):\n",
    "        return self.obs['pov']\n",
    "    \n",
    "    def close():\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: DQN on MineRL\n",
    "\n",
    "Now we're ready to test our DQN agent on our discretized Minecraft Navigation task. Again, if everything is implemented correctly, reward should go up within less than 3000 training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_env = DiscreteMinecraftNavigation()\n",
    "minerl_agent = QLearningAgent(nav_env.observation_space, \n",
    "                              nav_env.action_space,\n",
    "                              learning_rate=0.5,\n",
    "                              memory_size=10000,\n",
    "                              num_hidden=512)\n",
    "experiment = Experiment(nav_env, mine_rl, window_size=100, r_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment for 5000 steps, visualize every 50 steps\n",
    "# If implemented correctly, DQN should learn to exceed a reward of 0 within less than 5000 steps\n",
    "experiment.run(5000, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
